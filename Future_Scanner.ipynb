{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Future_Scanner",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPyH0S07JvhSbC+W6pmhtrY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EpicMahmoudSammourRTA/Future_Scanner/blob/main/Future_Scanner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlallrWc6knx"
      },
      "source": [
        "##install required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Usd8Q6oE6iTp"
      },
      "source": [
        "!pip install PyMuPDF\r\n",
        "!pip install fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AQL1mAD6WgU"
      },
      "source": [
        "##Import all packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhsRn84R562F"
      },
      "source": [
        "import pandas as pd\r\n",
        "import os\r\n",
        "import re\r\n",
        "import string\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.cluster.util import cosine_distance\r\n",
        "import numpy as np\r\n",
        "import fitz\r\n",
        "import string\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\r\n",
        "import fasttext\r\n",
        "import csv\r\n",
        "import random\r\n",
        "from os.path import join, dirname"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co1pVRAT6ayd"
      },
      "source": [
        "##Use the NLTK Downloader to obtain the resource"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frroYs_c59Dy"
      },
      "source": [
        "nltk.download('wordnet')\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfIpM--DoIzj"
      },
      "source": [
        "#To create the needed directories\r\n",
        "### please upload some pdf files in the directories "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwZSocQqnNXV"
      },
      "source": [
        "!mkdir /Output\r\n",
        "!mkdir /Files1\r\n",
        "!mkdir /Files2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htSS2LLk6dNp"
      },
      "source": [
        "##Set the following paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGIZbd3x5_Fr"
      },
      "source": [
        "\r\n",
        "Output_Path = '/Output'  # Output data path\r\n",
        "File1_Path = '/Files1' # Output data path\r\n",
        "File2_Path = '/Files2'  # Output data path\r\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw6TwD_c607V"
      },
      "source": [
        "# Load File set 1\r\n",
        "##Place all the files from File1 folder into File1_Path\r\n",
        "##pdf_reader function reads all the documents/articles in pdf format and combine in one dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Smt5xDvS6CjF"
      },
      "source": [
        "os.chdir(File1_Path)\r\n",
        "\r\n",
        "files = os.listdir(File1_Path)\r\n",
        "\r\n",
        "Key_Trend = []\r\n",
        "Trend = []\r\n",
        "File_Name = []\r\n",
        "\r\n",
        "for file in files:\r\n",
        "    Key_Trend.append(file.split('_')[0].replace('.pdf', ''))\r\n",
        "    Trend.append(file.split('_')[-1][:-4])\r\n",
        "    File_Name.append(file)\r\n",
        "\r\n",
        "file1 = pd.DataFrame(zip(Key_Trend, Trend, File_Name), columns=[\"Label\", \"Sub_Trend\", \"File_Name\"])\r\n",
        "\r\n",
        "\r\n",
        "def pdf_reader(rows):\r\n",
        "    page_text = ''\r\n",
        "    doc = fitz.open(rows)\r\n",
        "    for i in range(doc.pageCount):\r\n",
        "        page = doc.loadPage(i)\r\n",
        "        page_str = page.getText(\"text\")\r\n",
        "        page_text = page_text + page_str\r\n",
        "    return page_text\r\n",
        "\r\n",
        "\r\n",
        "file1[\"Raw_Data\"] = file1[\"File_Name\"].apply(pdf_reader)\r\n",
        "\r\n",
        "file1 = file1.loc[:, [\"Label\", \"File_Name\", \"Raw_Data\"]]\r\n",
        "file1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlkhLs9y7Oom"
      },
      "source": [
        "#Load File set 2\r\n",
        "##Place all the files from File2 folder into File2_Path\r\n",
        "##pdf_reader function reads all the documents/articles in pdf format and combine in one dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlvcG-Bi6GmD"
      },
      "source": [
        "os.chdir(File2_Path)\r\n",
        "\r\n",
        "files = os.listdir(File2_Path)\r\n",
        "\r\n",
        "Key_Trend1 = []\r\n",
        "Key_Trend2 = []\r\n",
        "Key_Trend3 = []\r\n",
        "Key_Trend4 = []\r\n",
        "File_Name = []\r\n",
        "\r\n",
        "for file in files:\r\n",
        "    file_list = file.split('__')\r\n",
        "    if len(file_list) == 2:\r\n",
        "        Key_Trend1.append(file_list[0])\r\n",
        "        Key_Trend2.append('')\r\n",
        "        Key_Trend3.append('')\r\n",
        "        Key_Trend4.append('')\r\n",
        "        File_Name.append(file)\r\n",
        "    elif len(file_list) == 3:\r\n",
        "        Key_Trend1.append(file_list[0])\r\n",
        "        Key_Trend2.append(file_list[1])\r\n",
        "        Key_Trend3.append('')\r\n",
        "        Key_Trend4.append('')\r\n",
        "        File_Name.append(file)\r\n",
        "    elif len(file_list) == 4:\r\n",
        "        Key_Trend1.append(file_list[0])\r\n",
        "        Key_Trend2.append(file_list[1])\r\n",
        "        Key_Trend3.append(file_list[2])\r\n",
        "        Key_Trend4.append('')\r\n",
        "        File_Name.append(file)\r\n",
        "    elif len(file_list) == 5:\r\n",
        "        Key_Trend1.append(file_list[0])\r\n",
        "        Key_Trend2.append(file_list[1])\r\n",
        "        Key_Trend3.append(file_list[2])\r\n",
        "        Key_Trend4.append(file_list[3])\r\n",
        "        File_Name.append(file)\r\n",
        "\r\n",
        "file2 = pd.DataFrame(zip(Key_Trend1, Key_Trend2, Key_Trend3, Key_Trend4, File_Name),\r\n",
        "                     columns=[\"Label1\", \"Label2\", \"Label3\", \"Label4\", \"File_Name\"])\r\n",
        "\r\n",
        "df1 = file2.loc[:, [\"Label1\", \"File_Name\"]]\r\n",
        "\r\n",
        "df1.rename(columns={\"Label1\": \"Label\"}, inplace=True)\r\n",
        "\r\n",
        "df2 = file2.loc[file2[\"Label2\"] != '', [\"Label2\", \"File_Name\"]]\r\n",
        "\r\n",
        "df2.rename(columns={\"Label2\": \"Label\"}, inplace=True)\r\n",
        "\r\n",
        "df3 = file2.loc[file2[\"Label3\"] != '', [\"Label3\", \"File_Name\"]]\r\n",
        "\r\n",
        "df3.rename(columns={\"Label3\": \"Label\"}, inplace=True)\r\n",
        "\r\n",
        "df4 = file2.loc[file2[\"Label4\"] != '', [\"Label4\", \"File_Name\"]]\r\n",
        "\r\n",
        "df4.rename(columns={\"Label4\": \"Label\"}, inplace=True)\r\n",
        "\r\n",
        "file2 = df1.append([df2, df3, df4], ignore_index=True)\r\n",
        "\r\n",
        "\r\n",
        "def pdf_reader(rows):\r\n",
        "    page_text = ''\r\n",
        "    doc = fitz.open(rows)\r\n",
        "    for i in range(doc.pageCount):\r\n",
        "        page = doc.loadPage(i)\r\n",
        "        page_str = page.getText(\"text\")\r\n",
        "        page_text = page_text + page_str\r\n",
        "    return page_text\r\n",
        "\r\n",
        "\r\n",
        "file2[\"Raw_Data\"] = file2[\"File_Name\"].apply(pdf_reader)\r\n",
        "\r\n",
        "base_data = file1.append([file2], ignore_index=True)\r\n",
        "base_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCxYp55O7cMp"
      },
      "source": [
        "#Oversampling to train the model\r\n",
        "#Oversampling is done for imbalance class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4fjoZDo6JLA"
      },
      "source": [
        "max_length = 60\r\n",
        "\r\n",
        "\r\n",
        "def oversampling(rows):\r\n",
        "    global max_length\r\n",
        "    df = base_data.loc[base_data[\"Label\"] == rows[\"Label\"], :]\r\n",
        "    ratio = len(df) / max_length\r\n",
        "    if ratio <= 0.40:\r\n",
        "        token_text = word_tokenize(rows[\"Raw_Data\"])\r\n",
        "        random.shuffle(token_text)\r\n",
        "        text = (\" \").join(token_text)\r\n",
        "        return text\r\n",
        "    else:\r\n",
        "        return \"\"\r\n",
        "\r\n",
        "\r\n",
        "wn = nltk.WordNetLemmatizer()\r\n",
        "\r\n",
        "base_data[\"Oversample_Text\"] = base_data[[\"Label\", \"Raw_Data\"]].apply(oversampling, axis=1)\r\n",
        "\r\n",
        "os_df = base_data.loc[base_data[\"Oversample_Text\"] != \"\", [\"Label\", \"File_Name\", \"Oversample_Text\"]]\r\n",
        "\r\n",
        "os_df.rename(columns={\"Oversample_Text\": \"Raw_Data\"}, inplace=True)\r\n",
        "\r\n",
        "base_data.drop([\"Oversample_Text\"], inplace=True, axis=1)\r\n",
        "\r\n",
        "base_data = base_data.append([os_df], ignore_index=True)\r\n",
        "\r\n",
        "base_data_v1 = base_data.loc[~(base_data[\"Label\"].isin(\r\n",
        "    [\"Novel Payment Systems\", \"Biometrics and Human-Machine Interface\", \"n_Self-Driving Transport\"])), :]\r\n",
        "\r\n",
        "base_data_v1[\"Label\"] = base_data_v1[\"Label\"].apply(lambda x: x.replace(' ', ''))\r\n",
        "\r\n",
        "base_data_v1['Label'] = ['__label__' + s for s in base_data_v1['Label']]\r\n",
        "\r\n",
        "base_data_v1 = base_data_v1.groupby(['Raw_Data'])['Label'].apply(' '.join).reset_index()\r\n",
        "\r\n",
        "file_name = base_data.drop_duplicates(subset=['Raw_Data'])\r\n",
        "\r\n",
        "base_data_v2 = base_data_v1.merge(file_name[['Raw_Data', \"File_Name\"]], left_on='Raw_Data', right_on='Raw_Data',\r\n",
        "                                  how='left')\r\n",
        "base_data_v2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I8_gik87mdM"
      },
      "source": [
        "#Finding Document Features\r\n",
        "#wc function is used to extract document features based on n-gram model and TF scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ejk3Jbn56Oux"
      },
      "source": [
        "def wc(text, tfidf_vectorizer):\r\n",
        "    doc_text = [text]\r\n",
        "    tf_model = tfidf_vectorizer.fit_transform(doc_text)\r\n",
        "    text_scored = tfidf_vectorizer.transform(doc_text)\r\n",
        "    terms = tfidf_vectorizer.get_feature_names()\r\n",
        "    scores = text_scored.toarray().flatten().tolist()\r\n",
        "    data = list(zip(terms, scores))\r\n",
        "    sorted_data = sorted(data, key=lambda x: x[1], reverse=True)\r\n",
        "    top_words = sorted_data[:2]\r\n",
        "    final_words = []\r\n",
        "    for i in range(len(top_words)):\r\n",
        "        final_words.append(top_words[i][0])\r\n",
        "    return final_words\r\n",
        "\r\n",
        "stop_words = stopwords.words('english')\r\n",
        "newStopWords = ['created', 'modified', 'scout', 'strat', 'alun', 'rhydderch', 'description', 'tags', 'trends',\r\n",
        "                'projects', 'page', 'steep', ]\r\n",
        "stop_words.extend(newStopWords)\r\n",
        "ps = nltk.PorterStemmer()\r\n",
        "wn = nltk.WordNetLemmatizer()\r\n",
        "stop_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L__xZ_7W7tTw"
      },
      "source": [
        "#Data Preprocessing\r\n",
        "#preprocessing function is used to cleansing the Raw data and will generate Clean_Text and Document_Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EiZvxUg6SRw"
      },
      "source": [
        "def preprocessing(rows):\r\n",
        "    text = rows.lower()\r\n",
        "    # /*********************Remove number*******************/\r\n",
        "    text = re.sub(r'\\d+', ' ', text)\r\n",
        "    # /*****************Remove Punctuation****************/\r\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\r\n",
        "    # /*****************Remove \\xa0****************/\r\n",
        "    text = re.sub(r'\\xa0', '', text)\r\n",
        "    # /*****************Remove \\x0c****************/\r\n",
        "    text = re.sub(r'\\x0c', '', text)\r\n",
        "    #    /*****************Remove stop words************/\r\n",
        "    token_text = word_tokenize(text)\r\n",
        "    tokens_without_sw = [word for word in token_text if not word in stop_words]\r\n",
        "    ##    text_lem = [wn.lemmatize(word) for word in tokens_without_sw]\r\n",
        "    text_stem = [ps.stem(word) for word in tokens_without_sw]\r\n",
        "    text = (\" \").join(text_stem)\r\n",
        "    # /***************Remove space line character*********/\r\n",
        "    text = text.replace('\\n', ' ')\r\n",
        "    #    /********************Remove duplicate space**********/\r\n",
        "    text = \" \".join(text.split())\r\n",
        "    # /********************************For word cloud**********************/\r\n",
        "    text_lem = [wn.lemmatize(word) for word in tokens_without_sw]\r\n",
        "    word_text = (\" \").join(text_lem)\r\n",
        "    # /***************Remove space line character*********/\r\n",
        "    word_text = word_text.replace('\\n', ' ')\r\n",
        "    #    /********************Remove duplicate space**********/\r\n",
        "    word_text = \" \".join(word_text.split())\r\n",
        "    tfidf_vectorizer = TfidfVectorizer()\r\n",
        "    top_unigram_words = wc(word_text, tfidf_vectorizer)\r\n",
        "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(2, 2))\r\n",
        "    top_bigram_words = wc(word_text, tfidf_vectorizer)\r\n",
        "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(3, 3))\r\n",
        "    top_trigram_words = wc(word_text, tfidf_vectorizer)\r\n",
        "    combined_gram = top_unigram_words + top_bigram_words + top_trigram_words\r\n",
        "    return text, combined_gram\r\n",
        "\r\n",
        "\r\n",
        "base_data_v2[\"Clean_Text\"], base_data_v2[\"Document_Features\"] = zip(*base_data_v2[\"Raw_Data\"].apply(preprocessing))\r\n",
        "base_data_v2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf4hD1F67zgk"
      },
      "source": [
        "#Model Building\r\n",
        "#We are using Fasttext to build our mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVHcYzCB7-TA"
      },
      "source": [
        "train_data = base_data_v2.loc[:, [\"Label\", \"Clean_Text\"]]\r\n",
        "\r\n",
        "train_data.to_csv(Output_Path + '\\\\train_data.txt', index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE,\r\n",
        "                  quotechar=\"\", escapechar=\" \")\r\n",
        "\r\n",
        "model = fasttext.train_supervised(input=Output_Path + '\\\\train_data.txt', lr=0.5, epoch=100, loss='hs')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XLVbFIE7_xI"
      },
      "source": [
        "#Model Predictions\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nFy9eHd6UFe"
      },
      "source": [
        "def predictions(rows):\r\n",
        "    predicted_label_1 = model.predict(rows, k=-1)[0][0]\r\n",
        "    predicted_label_1_probab = model.predict(rows, k=-1)[1][0]\r\n",
        "    predicted_label_2 = model.predict(rows, k=-1)[0][1]\r\n",
        "    predicted_label_2_probab = model.predict(rows, k=-1)[1][1]\r\n",
        "    try:\r\n",
        "        predicted_label_3 = model.predict(rows, k=-1)[0][2]\r\n",
        "        predicted_label_3_probab = model.predict(rows, k=-1)[1][2]\r\n",
        "    except:\r\n",
        "        predicted_label_3 = ''\r\n",
        "        predicted_label_3_probab = ''\r\n",
        "    if predicted_label_1_probab >= 0.80 or predicted_label_2_probab < 0.15:\r\n",
        "        return predicted_label_1, predicted_label_1_probab, ' ', ' ', ' ', ' '\r\n",
        "    elif (predicted_label_1_probab + predicted_label_2_probab) >= 0.70:\r\n",
        "        return predicted_label_1, predicted_label_1_probab, predicted_label_2, predicted_label_2_probab, ' ', ' '\r\n",
        "    else:\r\n",
        "        return predicted_label_1, predicted_label_1_probab, predicted_label_2, predicted_label_2_probab, predicted_label_3, predicted_label_3_probab\r\n",
        "\r\n",
        "\r\n",
        "base_data_v2[\"predicted_label_1\"], base_data_v2[\"predicted_label_1_probab\"], base_data_v2[\"predicted_label_2\"], \\\r\n",
        "base_data_v2[\"predicted_label_2_probab\"], base_data_v2[\"predicted_label_3\"], base_data_v2[\r\n",
        "    \"predicted_label_3_probab\"] = zip(*base_data_v2[\"Clean_Text\"].apply(predictions))\r\n",
        "base_data_v2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZfZe9tU8KPD"
      },
      "source": [
        "#This function is used to clean the model output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhKJMdqb5s0Y"
      },
      "source": [
        "def clean(rows):\r\n",
        "    text = rows.replace(\"__label__\", '')\r\n",
        "    return text\r\n",
        "\r\n",
        "\r\n",
        "base_data_v2[\"Label\"] = base_data_v2[\"Label\"].apply(clean)\r\n",
        "base_data_v2[\"predicted_label_1\"] = base_data_v2[\"predicted_label_1\"].apply(clean)\r\n",
        "base_data_v2[\"predicted_label_2\"] = base_data_v2[\"predicted_label_2\"].apply(clean)\r\n",
        "base_data_v2[\"predicted_label_3\"] = base_data_v2[\"predicted_label_3\"].apply(clean)\r\n",
        "\r\n",
        "base_data_v2.rename(columns={\"Label\": \"Actual_Label\"}, inplace=True)\r\n",
        "\r\n",
        "base_data_v3 = base_data_v2.loc[:,\r\n",
        "               [\"File_Name\", \"Actual_Label\", \"Document_Features\", \"predicted_label_1\", \"predicted_label_1_probab\",\r\n",
        "                \"predicted_label_2\", \"predicted_label_2_probab\", \"predicted_label_3\", \"predicted_label_3_probab\"]]\r\n",
        "base_data_v3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFWcS4qa8Mgx"
      },
      "source": [
        "#Export the output results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrJq-c_I8MsV"
      },
      "source": [
        "base_data_v3.to_excel(Output_Path + '\\\\Scoring_Data.xlsx')"
      ],
      "execution_count": 65,
      "outputs": []
    }
  ]
}